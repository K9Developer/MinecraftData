name: Get Minecraft Items

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  get_items:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.PAT }}
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
        pip install aiohttp
        pip install beautifulsoup4
        pip install Pillow

    - name: Create items directory
      run: mkdir -p items

    - name: Run items script
      run: |
        cat << EOT > get_items.py
        import asyncio
        import aiohttp
        from bs4 import BeautifulSoup
        import re
        from PIL import Image
        import io
        import os
        import json
        import time
        import math
        from aiohttp import ClientConnectorError, ServerDisconnectedError, ClientOSError
        
        image_cache = {}
        
        async def fetch(session, url, max_retries=5, delay=1):
            for attempt in range(max_retries):
                try:
                    async with session.get(url) as response:
                        return await response.text()
                except (ClientConnectorError, ServerDisconnectedError, ClientOSError, asyncio.TimeoutError) as e:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                        delay *= 2
                    else:
                        print(f"Failed to fetch {url} after {max_retries} attempts: {e}")
                        return None
        
        async def fetch_image(session, url, max_retries=5, delay=1):
            if url in image_cache:
                return image_cache[url]
        
            for attempt in range(max_retries):
                try:
                    async with session.get(url) as response:
                        image_data = await response.read()
                        image_cache[url] = image_data
                        return image_data
                except (ClientConnectorError, ServerDisconnectedError, ClientOSError, asyncio.TimeoutError) as e:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                        delay *= 2
                    else:
                        print(f"Failed to fetch image {url} after {max_retries} attempts: {e}")
                        return None
        
        async def process_entity(session, entity_li, entity_type):
            if entity_li.find("a") is None:
                return None, 1
            
            entity_name = entity_li.text
            entity_li_href = entity_li.find('a')['href'].replace("\\"", "").replace("\\\\", "")
            entity_url = f"https://minecraft.fandom.com{entity_li_href}"
            
            td = entity_li.find_parent("td")
            th = td.find_previous_sibling("th")
            name = th.text.strip().replace("\n", "")
            exclusive_editions = ["Bedrock Edition only", "Education Edition only", "April Fools", "Unimplemented", "Legacy Console Edition only", "Unused", "BE & EE only", "BE & edu only", "Non-physical", "Removed"]
            if any(edition in name for edition in exclusive_editions):
                print(f"[!] {entity_name.replace('\n', '').strip()} is blacklisted, skipping... ({entity_url})")
                return None, 1
        
            entity_data = await fetch(session, entity_url)
            entity_soup = BeautifulSoup(entity_data, "html.parser")
            
            entity_table = None
            for h2 in entity_soup.find_all("h2"):
                if re.sub(r'\W+', '', h2.text.strip()) == "Datavalues":
                    h3 = h2.find_next("h3")
                    if re.sub(r'\W+', '', h3.text.strip()) == "ID":
                        entity_table = h3.find_next("table")
                        break
            
            if entity_table is None:
                print(f"[!] Couldn't find {entity_type} table for {entity_name}, skipping...")
                return None, 1
        
            sub_entities = []
            rows = entity_table.find_all("tr")
            replace_name = rows[0].find("th").text
            for row in rows[1:]:
                cells = row.find_all("td")
                if len(cells) < 2 or cells[0].find("span") is None:
                    continue
        
                sub_entity_name = cells[0].text.strip()
                if sub_entity_name == "Item" or sub_entity_name == "Block":
                    sub_entity_name = replace_name
                entity_identifier = cells[1].text.strip().lower()
                if entity_identifier == "416":
                    print(f"[!] Skipping {entity_type} {sub_entity_name}")
                    continue
                entity_img_url = cells[0].find("span")["style"].split("url(")[1].split(")")[0]
                
                sub_entities.append({
                    "name": sub_entity_name,
                    "id": entity_identifier,
                    "img_url": entity_img_url,
                    "background_position": cells[0].find("span")["style"].split("background-position:")[1].split(";")[0],
                    "type": entity_type
                })
        
            return sub_entities, 0
        
        async def save_entity_image(session, entity, atlas_image, current_offset):
            image_content = await fetch_image(session, entity['img_url'])
            image_obj = Image.open(io.BytesIO(image_content))
        
            background_position = entity['background_position'].split(" ")
            background_position = (-int(background_position[0].replace("px", "")), -int(background_position[1].replace("px", "")))
            entity_image = image_obj.crop((background_position[0], background_position[1], background_position[0] + 16, background_position[1] + 16))
            
            atlas_image.paste(entity_image, current_offset)
            
            return {
                "id": entity['id'],
                "name": entity['name'],
                "type": entity['type'],
                "offsetX": current_offset[0],
                "offsetY": current_offset[1]
            }
        
        async def scrape_entities(entity_type, data_url):
            print(f"--- Scraping {entity_type} ---")
            async with aiohttp.ClientSession() as session:
                data = await fetch(session, data_url)
                soup = BeautifulSoup(data, "html.parser")
                
                failed = 0
                metadata = []
        
                if entity_type == "items":
                    elements = soup.find_all("li")
                else:  # blocks
                    allowed_tables = ["Natural", "Biota", "Building", "Utility", "Creative only", "Commands only"]
                    elements = [li for table in soup.find_all("table") 
                                if table.find("span", {"class": "navbox-title"}) and table.find("span", {"class": "navbox-title"}).text in allowed_tables
                                for tr in table.find_all("tr")[1:] 
                                for td in tr.find_all("td") 
                                for li in td.find_all("li")]
        
                tasks = [process_entity(session, element, entity_type) for element in elements]
                results = await asyncio.gather(*tasks)
        
                for sub_entities, fail_count in results:
                    failed += fail_count
                    if sub_entities:
                        metadata.extend(sub_entities)
        
                print(f"--- Scraping {entity_type} done ---")
                return metadata
        
        def remove_duplicates(data):
            seen = set()
            result = []
            for item in data:
                item_id = item.get('id')
                item_name = item.get('name')
                if item_id is not None and item_name is not None:
                    if item_id not in seen and item_name not in seen:
                        seen.add(item_id)
                        seen.add(item_name)
                        result.append(item)
            return result
        
        async def main():
            tasks = [
                scrape_entities("items", "https://minecraft.fandom.com/api.php?action=parse&format=json&prop=text%7Cmodules%7Cjsconfigvars&title=Category%3AItems&text=%7B%7B%3ATemplate%3AItems%2Fcontent%7D%7D"),
                scrape_entities("blocks", "https://minecraft.fandom.com/wiki/Category:Blocks")
            ]
            all_metadata = await asyncio.gather(*tasks)
            
            # Merge metadata from both tasks
            merged_metadata = []
            for metadata_list in all_metadata:
                merged_metadata.extend(metadata_list)
        
            # Remove duplicates based on 'id'
            unique_metadata = remove_duplicates(merged_metadata)
            
            # Create a single atlas for both items and blocks
            atlas_size = math.ceil(math.sqrt(len(unique_metadata))) * 16
            atlas_image = Image.new('RGBA', (atlas_size, atlas_size), (0, 0, 0, 0))
            
            atlas_metadata = []
            current_offset = [0, 0]
            
            async with aiohttp.ClientSession() as session:
                for entity in unique_metadata:
                    entity_metadata = await save_entity_image(session, entity, atlas_image, tuple(current_offset))
                    atlas_metadata.append(entity_metadata)
                    
                    current_offset[0] += 16
                    if current_offset[0] >= atlas_size:
                        current_offset[0] = 0
                        current_offset[1] += 16
        
                # Add Echo Shard to the atlas
                echo_shard_image = "https://static.wikia.nocookie.net/minecraft_gamepedia/images/7/7e/Echo_Shard_JE1_BE1.png"
                echo_shard_id = "echo_shard"
                echo_shard_name = "Echo Shard"
                echo_shard_image_content = await fetch_image(session, echo_shard_image)
                echo_shard_image_obj = Image.open(io.BytesIO(echo_shard_image_content))
                echo_shard_image_obj = echo_shard_image_obj.resize((16, 16), Image.LANCZOS)
                atlas_image.paste(echo_shard_image_obj, tuple(current_offset))
                atlas_metadata.append({
                    "id": echo_shard_id,
                    "name": echo_shard_name,
                    "type": "items",
                    "offsetX": current_offset[0],
                    "offsetY": current_offset[1]
                })
                
        
            atlas_image.save("items/atlas.png")
            print("[+] Saved atlas")
        
            # Write merged metadata to file
            with open("items/atlas_metadata.json", "w") as f:
                json.dump(atlas_metadata, f, indent=2)
            
            print("[+] Done writing atlas metadata to file")
        
        if __name__ == "__main__":
            start_time = time.time()
            asyncio.run(main())
            print(f"Total execution time: {time.time() - start_time:.2f} seconds")
        EOT
        python get_items.py

    - name: Commit and push changes
      env:
        PAT: ${{ secrets.PAT }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add items/
        git commit -m "Update items data" -a || echo "No changes to commit"
        git push https://${PAT}@github.com/${GITHUB_REPOSITORY}.git HEAD:${GITHUB_REF}
