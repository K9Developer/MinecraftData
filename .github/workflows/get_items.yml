name: Get Minecraft Items

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  get_items:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.PAT }}
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
        pip install aiohttp
        pip install beautifulsoup4
        pip install Pillow

    - name: Create items directory
      run: mkdir -p items

    - name: Run items script
      run: |
        cat << EOT > get_items.py
        import asyncio
        import aiohttp
        from bs4 import BeautifulSoup
        import re
        from PIL import Image
        import io
        import os
        import json
        import time
        
        async def fetch(session, url):
            async with session.get(url) as response:
                return await response.text()
        
        async def fetch_image(session, url):
            async with session.get(url) as response:
                return await response.read()
        
        async def process_item(session, item_li, image_url, image_obj):
            if item_li.find("a") is None:
                return None, 1
            
            item_name = item_li.text
            item_url = item_li.find("a")["href"]
            item_url = f"https://minecraft.fandom.com{item_url}"
            
            # Get the td element containing the item_li (parent)
            td = item_li.find_parent("td")
            th = td.find_previous_sibling("th")
            name = th.text.strip().replace("\n", "")
            if "Bedrock Edition only" in name or \
                "Education Edition only" in name or \
                "April Fools" in name or \
                "Unimplemented" in name or \
                "Legacy Console Edition only" in name or \
                "Unused" in name or \
                "BE & EE only" in name:
                    print(f"[!] {item_li.text.replace("\n", "").strip()} is exclusive to another edition, skipping... ({item_url})")
                    return None, 1
        
        
        
            item_data = await fetch(session, item_url)
            item_soup = BeautifulSoup(item_data, "html.parser")
            
            item_table = None
            for h2 in item_soup.find_all("h2"):
                h2text = re.sub(r'\W+', '', h2.text.strip())
                if h2text == "Datavalues":
                    h3 = h2.find_next("h3")
                    h3text = re.sub(r'\W+', '', h3.text.strip())
                    if h3text == "ID":
                        item_table = h3.find_next("table")
                        break
            
            if item_table is None:
                print(f"[!] Couldn't find item table for {item_name}, skipping...")
                return None, 1
        
            sub_items = []
            rows = item_table.find_all("tr")
            for row in rows[1:]:
                if row.find("td") is None:
                    continue
        
                cells = row.find_all("td")
        
                if cells[0].find("span") is None:
                    continue
        
                sub_item_name = cells[0].text.strip()
                item_identifier = cells[1].text.strip().lower()
                if item_identifier == "416":
                    print(f"[!] Skipping item {sub_item_name}")
                    continue
                item_img_url = cells[0].find("span")["style"].split("url(")[1].split(")")[0]
                
                sub_items.append({
                    "name": sub_item_name,
                    "id": item_identifier,
                    "img_url": item_img_url,
                    "background_position": cells[0].find("span")["style"].split("background-position:")[1].split(";")[0]
                })
        
            return sub_items, 0
        
        async def save_image(session, item, image_url, image_obj):
            if os.path.exists(f"items/{item['id']}.png"):
                print(f"[?] Item {item['id']} already exists, skipping...")
                return
        
            if image_url != item['img_url']:
                image_content = await fetch_image(session, item['img_url'])
                image_obj = Image.open(io.BytesIO(image_content))
                image_url = item['img_url']
        
            background_position = item['background_position'].split(" ")
            background_position = (-int(background_position[0].replace("px", "")), -int(background_position[1].replace("px", "")))
            item_image = image_obj.crop((background_position[0], background_position[1], background_position[0] + 16, background_position[1] + 16))
            item_image.save(f"items/{item['id']}.png")
            print(f"[*] Saved sub item: {item['name']}")
        
        async def main():
            data_url = "https://minecraft.fandom.com/api.php?action=parse&format=json&prop=text%7Cmodules%7Cjsconfigvars&title=Category%3AItems&text=%7B%7B%3ATemplate%3AItems%2Fcontent%7D%7D"
            
            async with aiohttp.ClientSession() as session:
                data = await fetch(session, data_url)
                data = json.loads(data)
                data = data["parse"]["text"]["*"]
                soup = BeautifulSoup(data, "html.parser")
                
                failed = 0
                image_url = None
                image_obj = None
                metadata = []
        
                print("--- Scraping items ---")
                tasks = [process_item(session, item_li, image_url, image_obj) for item_li in soup.find_all("li")]
                results = await asyncio.gather(*tasks)
        
                for sub_items, fail_count in results:
                    failed += fail_count
                    if sub_items:
                        metadata.extend(sub_items)
        
                image_tasks = [save_image(session, item, image_url, image_obj) for item in metadata]
                await asyncio.gather(*image_tasks)
        
                print("--- Scraping items done ---")
            
                echo_shard_image = "https://static.wikia.nocookie.net/minecraft_gamepedia/images/7/7e/Echo_Shard_JE1_BE1.png"
                echo_shard_id = "echo_shard"
                echo_shard_name = "Echo Shard"
                echo_shard_image_content = await fetch_image(session, echo_shard_image)
                echo_shard_image_obj = Image.open(io.BytesIO(echo_shard_image_content))
                echo_shard_image_obj.save(f"items/{echo_shard_id}.png")
                metadata.append({
                    "name": echo_shard_name,
                    "id": echo_shard_id,
                })
            
                print("[+] Writing metadata to file...")
                for item in metadata:
                    item.pop("background_position", None)
                    item.pop("img_url", None)
                with open("items/metadata.json", "w") as f:
                    json.dump(metadata, f)
                print("[+] Done writing metadata to file")
                print(f"[!] Failed to scrape {failed} items")
        
        if __name__ == "__main__":
            start_time = time.time()
            asyncio.run(main())
            print(f"Total execution time: {time.time() - start_time:.2f} seconds")
        EOT
        python get_items.py

    - name: Commit and push changes
      env:
        PAT: ${{ secrets.PAT }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add items/
        git commit -m "Update items data" -a || echo "No changes to commit"
        git push https://${PAT}@github.com/${GITHUB_REPOSITORY}.git HEAD:${GITHUB_REF}
