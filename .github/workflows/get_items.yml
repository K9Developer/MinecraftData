name: Get Minecraft Items

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  get_items:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.PAT }}
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
        pip install aiohttp
        pip install beautifulsoup4
        pip install Pillow

    - name: Create items directory
      run: mkdir -p items

    - name: Run items script
      run: |
        cat << EOT > get_items.py
        import asyncio
        import aiohttp
        from bs4 import BeautifulSoup
        import re
        from PIL import Image
        import io
        import os
        import json
        import time
        
        import asyncio
        import aiohttp
        from aiohttp import ClientConnectorError, ServerDisconnectedError, ClientOSError
        
        image_cache = {}
        
        async def fetch(session, url, max_retries=5, delay=1):
            for attempt in range(max_retries):
                try:
                    async with session.get(url) as response:
                        return await response.text()
                except (ClientConnectorError, ServerDisconnectedError, ClientOSError, asyncio.TimeoutError) as e:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                        delay *= 2
                    else:
                        print(f"Failed to fetch {url} after {max_retries} attempts: {e}")
                        return None
        
        async def fetch_image(session, url, max_retries=5, delay=1):
            # Check if the image is already in the cache
            if url in image_cache:
                return image_cache[url]
        
            for attempt in range(max_retries):
                try:
                    async with session.get(url) as response:
                        image_data = await response.read()
                        # Cache the image data
                        image_cache[url] = image_data
                        return image_data
                except (ClientConnectorError, ServerDisconnectedError, ClientOSError, asyncio.TimeoutError) as e:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                        delay *= 2
                    else:
                        print(f"Failed to fetch image {url} after {max_retries} attempts: {e}")
                        return None
        
        async def process_entity(session, entity_li, entity_type):
            if entity_li.find("a") is None:
                return None, 1
            
            entity_name = entity_li.text
            entity_li_href = entity_li.find('a')['href'].replace("\\"", "").replace("\\\\", "")
            entity_url = f"https://minecraft.fandom.com{entity_li_href}"
            
            # Check for exclusivity
            td = entity_li.find_parent("td")
            th = td.find_previous_sibling("th")
            name = th.text.strip().replace("\n", "")
            exclusive_editions = ["Bedrock Edition only", "Education Edition only", "April Fools", "Unimplemented", "Legacy Console Edition only", "Unused", "BE & EE only", "BE & edu only", "Non-physical", "Removed"]
            if any(edition in name for edition in exclusive_editions):
                print(f"[!] {entity_name.replace('\n', '').strip()} is blacklisted, skipping... ({entity_url})")
                return None, 1
        
            entity_data = await fetch(session, entity_url)
            entity_soup = BeautifulSoup(entity_data, "html.parser")
            
            entity_table = None
            for h2 in entity_soup.find_all("h2"):
                if re.sub(r'\W+', '', h2.text.strip()) == "Datavalues":
                    h3 = h2.find_next("h3")
                    if re.sub(r'\W+', '', h3.text.strip()) == "ID":
                        entity_table = h3.find_next("table")
                        break
            
            if entity_table is None:
                print(f"[!] Couldn't find {entity_type} table for {entity_name}, skipping...")
                return None, 1
        
            sub_entities = []
            rows = entity_table.find_all("tr")
            for row in rows[1:]:
                cells = row.find_all("td")
                if len(cells) < 2 or cells[0].find("span") is None:
                    continue
        
                sub_entity_name = cells[0].text.strip()
                entity_identifier = cells[1].text.strip().lower()
                if entity_identifier == "416":
                    print(f"[!] Skipping {entity_type} {sub_entity_name}")
                    continue
                entity_img_url = cells[0].find("span")["style"].split("url(")[1].split(")")[0]
                
                sub_entities.append({
                    "name": sub_entity_name,
                    "id": entity_identifier,
                    "img_url": entity_img_url,
                    "background_position": cells[0].find("span")["style"].split("background-position:")[1].split(";")[0]
                })
        
            return sub_entities, 0
        
        async def save_entity_image(session, entity, folder):
            if os.path.exists(f"items/{entity['id']}.png"):
                print(f"[?] {folder.capitalize()} {entity['id']} already exists, skipping...")
                return
        
            image_content = await fetch_image(session, entity['img_url'])
            image_obj = Image.open(io.BytesIO(image_content))
        
            background_position = entity['background_position'].split(" ")
            background_position = (-int(background_position[0].replace("px", "")), -int(background_position[1].replace("px", "")))
            entity_image = image_obj.crop((background_position[0], background_position[1], background_position[0] + 16, background_position[1] + 16))
            entity_image.save(f"items/{entity['id']}.png")
            print(f"[*] Saved {folder} {entity['name']}")
        
        async def scrape_entities(entity_type, data_url):
            print(f"--- Scraping {entity_type} ---")
            async with aiohttp.ClientSession() as session:
                data = await fetch(session, data_url)
                soup = BeautifulSoup(data, "html.parser")
                
                failed = 0
                metadata = []
        
                if entity_type == "items":
                    elements = soup.find_all("li")
                else:  # blocks
                    allowed_tables = ["Natural", "Biota", "Building", "Utility", "Creative only", "Commands only"]
                    elements = [li for table in soup.find_all("table") 
                                if table.find("span", {"class": "navbox-title"}) and table.find("span", {"class": "navbox-title"}).text in allowed_tables
                                for tr in table.find_all("tr")[1:] 
                                for td in tr.find_all("td") 
                                for li in td.find_all("li")]
        
                tasks = [process_entity(session, element, entity_type) for element in elements]
                results = await asyncio.gather(*tasks)
        
                for sub_entities, fail_count in results:
                    failed += fail_count
                    if sub_entities:
                        metadata.extend(sub_entities)
        
                image_tasks = [save_entity_image(session, entity, entity_type) for entity in metadata]
                await asyncio.gather(*image_tasks)
        
                print(f"--- Scraping {entity_type} done ---")
            
                echo_shard_image = "https://static.wikia.nocookie.net/minecraft_gamepedia/images/7/7e/Echo_Shard_JE1_BE1.png"
                echo_shard_id = "echo_shard"
                echo_shard_name = "Echo Shard"
                echo_shard_image_content = await fetch_image(session, echo_shard_image)
                echo_shard_image_obj = Image.open(io.BytesIO(echo_shard_image_content))
                echo_shard_image_obj.save(f"items/{echo_shard_id}.png")
                metadata.append({
                        "name": echo_shard_name,
                        "id": echo_shard_id,
                    })
                for entity in metadata:
                    entity.pop("background_position", None)
                    entity.pop("img_url", None)
                print(f"[+] Writing {entity_type} metadata to file...")
                for entity in metadata:
                    entity.pop("background_position", None)
                    entity.pop("img_url", None)
                
                with open(f"items/metadata.json", "w+") as f:
                    existing_metadata = []
                    try:
                        existing_metadata = json.load(f)
                    except json.JSONDecodeError:
                        pass
                    existing_metadata.extend(metadata)
                    f.write(json.dumps(existing_metadata))
        
                print(f"[+] Done writing {entity_type} metadata to file")
                print(f"[!] Failed to scrape {failed} {entity_type}")
        
        async def main():
            # 720 + 820 = 1540
            tasks = [
                scrape_entities("items", "https://minecraft.fandom.com/api.php?action=parse&format=json&prop=text%7Cmodules%7Cjsconfigvars&title=Category%3AItems&text=%7B%7B%3ATemplate%3AItems%2Fcontent%7D%7D"),
                scrape_entities("blocks", "https://minecraft.fandom.com/wiki/Category:Blocks")
            ]
            await asyncio.gather(*tasks)
        
        if __name__ == "__main__":
            start_time = time.time()
            asyncio.run(main())
            print(f"Total execution time: {time.time() - start_time:.2f} seconds")
        EOT
        python get_items.py

    - name: Commit and push changes
      env:
        PAT: ${{ secrets.PAT }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add items/
        git commit -m "Update items data" -a || echo "No changes to commit"
        git push https://${PAT}@github.com/${GITHUB_REPOSITORY}.git HEAD:${GITHUB_REF}
